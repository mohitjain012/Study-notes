{\rtf1\ansi\ansicpg1252\deff0\deflang1033\deflangfe1033{\fonttbl{\f0\fswiss\fprq2\fcharset0 Calibri;}{\f1\froman\fprq2\fcharset0 Times New Roman;}{\f2\fswiss\fprq2\fcharset0 Arial;}{\f3\fnil\fcharset2 Symbol;}}
{\colortbl ;\red68\green68\blue68;\red255\green255\blue255;}
{\*\generator Msftedit 5.41.21.2510;}\viewkind4\uc1\pard\nowidctlpar\ul\b\f0\fs22 56.Directed Acyclic Graph (DAG) and overview of Transformations and Actions\b0\par
\ulnone\par
Each task apply trasnformation/action.\par
Trasnformation/action are a piece of code which perform certain activity on data.\par
All spark transformations do lazy evaluation.\par
DAG is the workflow of the program.\par
Actions execute the DAG.\b\par
\par
Lazy evaluation:\b0\par
scala> val a = Thread.sleep(4000)\tab <- took 4 sec\par
a: Unit = ()\par
scala> lazy val a = Thread.sleep(4000)\par
a: Unit = <\b lazy\b0 >\par
Note:  a is lazy so it will not be evaluated now but will be evaluated later when it will be used first in action.\par
scala> a\tab\tab <- took 4 sec\par
Here a is evaluated and took 4 sec to exectute\par
scala> a\tab\tab <- didn't took 4 sec\par
As a is already evaluated so it will not evaluate again and return immediately\par
\par
scala> val a = 1 to 1000000\par
a: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 1...\par
\par
scala> val rdd = sc.parallelize(a)\par
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:29\par
\par
scala> rdd.toDebugString\tab <-\tab details about DAG\par
res6: String = (1) ParallelCollectionRDD[13] at parallelize at <console>:29 []\par
\par
\b Filter all odd number:\b0\par
scala> val rddFilter = rdd.filter(ele => ele%2==0)\par
rddFilter: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[14] at filter at <console>:31\par
scala> rddFilter.take(10)\par
res7: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)\par
Apply double on these numbers in rdd a:\par
scala> val rddMap = rddFilter.map(ele => ele*2)\par
rddMap: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[16] at map at <console>:33\par
scala> rddFilter.toDebugString\par
res8: String = \par
(1) MapPartitionsRDD[14] at filter at <console>:31 []\par
 |  ParallelCollectionRDD[13] at parallelize at <console>:29 []\par
scala> rddMap.toDebugString\par
res10: String = \par
(1) MapPartitionsRDD[16] at map at <console>:33 []\par
 |  MapPartitionsRDD[14] at filter at <console>:31 []\par
 |  ParallelCollectionRDD[13] at parallelize at <console>:29 []\par
scala> rddMap.take(10)\par
res11: Array[Int] = Array(4, 8, 12, 16, 20, 24, 28, 32, 36, 40)\par
\par
scala> rddMap.reduce((total,ele) => total+ele)\par
res12: Int = 1784793664\par
\par
\ul\b 57.Actions and Transformations - Lesson\ulnone\b0\par
\par
Here is the life cycle of Spark applications:\par
\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent0{\pntxtb\'B7}}\nowidctlpar\fi-360\li720 Identify file system\par
{\pntext\f3\'B7\tab}Understand file format and structure of data (including delimiters \endash  if applicable) and use appropriate API to read the data. Data will be typically stored in distributed in-memory collection called RDD.\par
{\pntext\f3\'B7\tab}textFile, sequenceFile, objectFile, hadoopFile, newAPIHadoopFile are the functions on Spark Context to read data from different file formats\par
{\pntext\f3\'B7\tab}Spark also have other APIs to read data from files, which are not covered yet.\par
{\pntext\f3\'B7\tab}Apply one or more transformations on the data\par
{\pntext\f3\'B7\tab}Use appropriate Action to write the data back to underlying file system\par
{\pntext\f3\'B7\tab}Repeat what ever is applicable to process the data as per requirement\par
{\pntext\f3\'B7\tab}One can perform these actions and transformations on local spark setup or virtual machines or cloud based labs provided by us\par
\pard\nowidctlpar\par
\b Actions\b0\par
\pard\nowidctlpar\li720 Actions are the ones which will execute the DAG\par
converting to array and preview data \endash  collect, take, first, takeSample\par
aggregate \endash  reduce, count\par
sorting and ranking \endash  top, takeOrdered\par
saving \endash  saveAsTextFile, saveAsSequenceFile\par
\pard\nowidctlpar\b Transformations\b0\par
\pard\nowidctlpar\li720 Transformations are the ones which will define the logic to process the data\par
Transformations can be row level, aggregations, joins, set operations etc\par
mapping \endash  map, flatMap, filter, mapPartitions\par
aggregate \endash  reduceByKey, aggregateByKey, groupByKey\par
joins \endash  join, leftOuterJoin, rightOuterJoin, cogroup, cartesian\par
set operations \endash  union, intersection\par
sorting and ranking \endash  sortByKey, groupByKey\par
controlling RDD partitions \endash  coalesce, repartition\par
\pard\nowidctlpar\par
local filesystem data path: /home/cloudera/Desktop/data/retail_db\par
HDFS path: /user/cloudera/retail_db/\par
\par
scala> val path = "retail_db"\par
path: String = retail_db\par
scala> val rdd = sc.textFile(path+"/orders")\par
rdd: org.apache.spark.rdd.RDD[String] = retail_db/orders MapPartitionsRDD[18] at textFile at <console>:29\par
scala> rdd.first\par
res13: String = 1,2013-07-25 00:00:00.0,11599,CLOSED\tab <-order_id, order_date, order_customer_id, order_status\par
\par
scala> rdd.take(10)\par
res14: Array[String] = Array(1,2013-07-25 00:00:00.0,11599,CLOSED, 2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT, 3,2013-07-25 00:00:00.0,12111,COMPLETE, 4,2013-07-25 00:00:00.0,8827,CLOSED, 5,2013-07-25 00:00:00.0,11318,COMPLETE, 6,2013-07-25 00:00:00.0,7130,COMPLETE, 7,2013-07-25 00:00:00.0,4530,COMPLETE, 8,2013-07-25 00:00:00.0,2911,PROCESSING, 9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT, 10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT)\par
\par
\b Collect()\b0  convert the data set into array, if you fire collect on a dataset which is distributed, then it will be created on one machine(driver) and run out of memory issue. So shouldn't be used.\par
\par
\b Find order with minimum order_sust_id in top 10 records:\b0\par
scala> val a = rdd.take(10)\par
a: Array[String] = Array(1,2013-07-25 00:00:00.0,11599,CLOSED, 2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT, 3,2013-07-25 00:00:00.0,12111,COMPLETE, 4,2013-07-25 00:00:00.0,8827,CLOSED, 5,2013-07-25 00:00:00.0,11318,COMPLETE, 6,2013-07-25 00:00:00.0,7130,COMPLETE, 7,2013-07-25 00:00:00.0,4530,COMPLETE, 8,2013-07-25 00:00:00.0,2911,PROCESSING, 9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT, 10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT)\par
\par
scala> var agg = a(0)\par
agg: String = 1,2013-07-25 00:00:00.0,11599,CLOSED\par
\par
scala> for(ele <- a)\{\par
     | if(agg.split(",")(2).toInt < ele.split(",")(2).toInt)\par
     |  agg = agg\par
     | else\par
     |  agg = ele\par
     | \}\par
\par
scala> agg\par
res18: String = 2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT\par
\par
scala> \par
\par
We shouldn't use loops(imperative method) like this in scala as its a functional programming. We should use inbuilt function wherever possible.\par
\par
scala> rdd.reduce\par
                                def reduce(f: (T, T) => T): T\par
\par
scala> rdd.reduce((agg,ele) => if(agg.split(",")(2).toInt < ele.split(",")(2).toInt) agg else ele)\par
res19: String = 22945,2013-12-13 00:00:00.0,1,COMPLETE\par
\par
scala> a.reduce((agg,ele) => if(agg.split(",")(2).toInt < ele.split(",")(2).toInt) agg else ele)\par
res20: String = 2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT\par
\par
Similarly reduce can be used to get sum,mix,max etc..\par
\par
rdd.top(2) - sort the data in descending order on which we apply it.\par
\par
\b It took first index values as i/p, since 9999 and 9998 are the max index values.\b0\par
scala> rdd.top(2)\par
res21: Array[String] = Array(9999,2013-09-25 00:00:00.0,1185,CLOSED, 9998,2013-09-25 00:00:00.0,9419,PENDING)\par
\par
\b Sorting on the basis of 3rd field:\b0\par
scala> rdd.map(rec => rec.split(",")(2).toInt).top(2)\par
res22: Array[Int] = Array(12435, 12435)\par
\par
\b Reverse in reverse order can be done using takeOrdered(any sort logic can be applied):\b0\par
scala> rdd.takeOrdered\par
                                                                            def takeOrdered(num: Int)(implicit ord: scala.math.Ordering[T]): Array[T]\par
\b\tab\tab\tab (no)(logic of sorting)\b0\par
scala> \b rdd.takeOrdered(5)(Ordering[Int].reverse.on(k => k.split(",")(2).toInt))\b0 .foreach(println)\par
41643,2014-04-08 00:00:00.0,12435,PENDING\par
61629,2013-12-21 00:00:00.0,12435,CANCELED\par
6160,2013-09-02 00:00:00.0,12434,COMPLETE\par
1868,2013-08-03 00:00:00.0,12434,CLOSED\par
13544,2013-10-16 00:00:00.0,12434,PENDING\par
\par
scala> rdd.takeOrdered(5).foreach(println)\par
1,2013-07-25 00:00:00.0,11599,CLOSED\par
10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT\par
100,2013-07-25 00:00:00.0,12131,PROCESSING\par
1000,2013-07-30 00:00:00.0,2321,CLOSED\par
10000,2013-09-25 00:00:00.0,8983,PROCESSING\par
\par
\b 60.Spark - Row level transformations - filter\b0\par
scala> val orders = sc.textFile("retail_db/orders")\par
orders: org.apache.spark.rdd.RDD[String] = retail_db/orders MapPartitionsRDD[26] at textFile at <console>:27\par
\par
scala> orders.first\par
res26: String = 1,2013-07-25 00:00:00.0,11599,CLOSED\tab <- order_id, order_date, order_cust_id, order_status\par
\par
Filter: keeps TRUE condition and eliminate FALSE condition values.\par
We want to keep on COMPLETE status orders and discard others:\par
scala> val completedOrders = orders.filter\par
                                      def filter(f: T => Boolean): RDD[T]  \par
\par
scala> val completedOrders = orders.filter(order => order.split(",")(3) == "COMPLETE")\par
completedOrders: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at filter at <console>:29\par
\par
scala> val completedOrders = orders.filter((order:String) => order.split(",")(3) == "COMPLETE")\par
completedOrders: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[28] at filter at <console>:29\par
\par
In scala == compares attributes of object not the object references.\par
Here we need not write if else, just TRUE condition.\par
\par
\b Filter such that order_status \ldblquote PENDING\rdblquote , \ldblquote PENDING_PAYMENT\rdblquote  and \ldblquote PROCESSING\rdblquote  remains there:\par
\b0 scala> val pendingOrders = orders.filter(order => order.split(",")(3).contains("PENDING") || order.split(",")(3) == "PROCESSING")\par
pendingOrders: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[29] at filter at <console>:29\par
\par
scala> pendingOrders.take(10).foreach(println)\par
2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT\par
8,2013-07-25 00:00:00.0,2911,PROCESSING\par
9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT\par
10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT\par
13,2013-07-25 00:00:00.0,9149,PENDING_PAYMENT\par
14,2013-07-25 00:00:00.0,9842,PROCESSING\par
16,2013-07-25 00:00:00.0,7276,PENDING_PAYMENT\par
19,2013-07-25 00:00:00.0,9488,PENDING_PAYMENT\par
20,2013-07-25 00:00:00.0,9198,PROCESSING\par
21,2013-07-25 00:00:00.0,2711,PENDING\par
\par
\b Filter such that order_status \ldblquote PENDING\rdblquote , \ldblquote PENDING_PAYMENT\rdblquote  and \ldblquote PROCESSING\rdblquote  of date august 2013 remains there:\par
\b0 scala> val pendingOrders = orders.filter(order => (order.split(",")(3).contains("PENDING") || order.split(",")(3) == "PROCESSING") && order.split(",")(1).contains("2013-08"))\par
pendingOrders: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[30] at filter at <console>:29\par
\par
scala> pendingOrders.take(10).foreach(println)\par
1300,2013-08-01 00:00:00.0,553,PENDING_PAYMENT\par
1301,2013-08-01 00:00:00.0,1604,PENDING_PAYMENT\par
1303,2013-08-01 00:00:00.0,7018,PROCESSING\par
1306,2013-08-01 00:00:00.0,11672,PENDING_PAYMENT\par
1308,2013-08-01 00:00:00.0,11645,PENDING\par
1310,2013-08-01 00:00:00.0,5602,PENDING\par
1311,2013-08-01 00:00:00.0,5396,PENDING_PAYMENT\par
1316,2013-08-01 00:00:00.0,6376,PENDING_PAYMENT\par
1317,2013-08-01 00:00:00.0,5467,PENDING\par
1325,2013-08-01 00:00:00.0,10425,PENDING\par
\b 61.Spark - Row level transformations - map and flatMap\par
Get order_status COMPLETE orders with only two fields 1. Order_id 2. Date in a Tuple:\par
\b0 I am already taking complete filtered orders here\par
scala> completedOrders.take(10).foreach(println)\par
3,2013-07-25 00:00:00.0,12111,COMPLETE\par
5,2013-07-25 00:00:00.0,11318,COMPLETE\par
6,2013-07-25 00:00:00.0,7130,COMPLETE\par
7,2013-07-25 00:00:00.0,4530,COMPLETE\par
15,2013-07-25 00:00:00.0,2568,COMPLETE\par
17,2013-07-25 00:00:00.0,2667,COMPLETE\par
22,2013-07-25 00:00:00.0,333,COMPLETE\par
26,2013-07-25 00:00:00.0,7562,COMPLETE\par
28,2013-07-25 00:00:00.0,656,COMPLETE\par
32,2013-07-25 00:00:00.0,3960,COMPLETE\par
\par
scala> val orderDates = completedOrders.map\par
     def map[U](f: T => U)(implicit scala.reflect.ClassTag[U]): RDD[U]\par
\par
scala> val orderDates = completedOrders.map( order => (order.split(",")(0).toInt, order.split(",")(1)))\par
orderDates: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[31] at map at <console>:31\par
\par
scala> orderDates.take(10).foreach(println)\par
(3,2013-07-25 00:00:00.0)\par
(5,2013-07-25 00:00:00.0)\par
(6,2013-07-25 00:00:00.0)\par
(7,2013-07-25 00:00:00.0)\par
(15,2013-07-25 00:00:00.0)\par
(17,2013-07-25 00:00:00.0)\par
(22,2013-07-25 00:00:00.0)\par
(26,2013-07-25 00:00:00.0)\par
(28,2013-07-25 00:00:00.0)\par
(32,2013-07-25 00:00:00.0)\par
\par
Let us map these scenarios with APIs categorized under transformations\par
\par
\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent0{\pntxtb\'B7}}\nowidctlpar\fi-360\li720 Data cleansing and standardization \endash  map. It takes one record as input and returns exactly one record as output\par
{\pntext\f3\'B7\tab}Discarding or filtering \endash  filter. It take one record as input, and if expression returns false record will be discarded\par
{\pntext\f3\'B7\tab}Also for vertical filtering, we use map function\par
{\pntext\f3\'B7\tab}Unpivoting \endash  flatMap. For each input record there will be 1 to n output records\par
{\pntext\f3\'B7\tab}Number of invocations = Number of elements in RDD\par
\pard\nowidctlpar\par
scala> val lines = Array("Hello World", \par
     |   "In this case we are trying to understand", \par
     |   "the purpose of flatMap", \par
     |   "flatMap is a function which will apply transformation", \par
     |   "if the transformation results in array, it will flatten out array as individual records", \par
     |   "let us also understand difference between map and flatMap", \par
     |   "in case of map, it take one record and return one record after applying transformation", \par
     |   "even if the transformation result in an array", \par
     |   "where as in case of flatMap, it might return one or more records", \par
     |   "if the transformation of 1 record result an array of 1000 records, ", \par
     |   "then flatMap returns 1000 records")\par
lines: Array[String] = Array(Hello World, In this case we are trying to understand, the purpose of flatMap, flatMap is a function which will apply transformation, if the transformation results in array, it will flatten out array as individual records, let us also understand difference between map and flatMap, in case of map, it take one record and return one record after applying transformation, even if the transformation result in an array, where as in case of flatMap, it might return one or more records, "if the transformation of 1 record result an array of 1000 records, ", then flatMap returns 1000 records)\par
\par
scala> val linesRDD = sc.parallelize(lines)\par
linesRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[32] at parallelize at <console>:29\par
\par
scala> lines.size\par
res40: Int = 11\par
\par
we need each word separately, so if we \par
\par
scala> linesRDD.map(line => line.split(" ")).count\tab <- map will convert into array\par
res44: Long = 11\par
\par
scala> linesRDD.flatMap(line => line.split(" ")).count\tab <- flatMap converts into individual records\par
res45: Long = 98\par
\par
scala> linesRDD.flatMap(line => line.split(" ")).collect.foreach(println)\par
Hello\par
World\par
In\par
this\par
...\par
\par
scala> linesRDD.flatMap\par
                                                                                         \par
def flatMap[U](f: T => TraversableOnce[U])(implicit scala.reflect.ClassTag[U]): RDD[U] \par
\par
\b 62.Spark Aggregations reduceByKey and aggregateByKey\par
\b0\par
\trowd\trgaph15\trleft-55\trrh481\trbrdrl\brdrs\brdrw10 \trbrdrt\brdrs\brdrw10 \trbrdrr\brdrs\brdrw10 \trbrdrb\brdrs\brdrw10 \trpaddl15\trpaddr15\trpaddfl3\trpaddfr3
\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx2874\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx6169\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx9291\pard\intbl\sl480\slmult1\cf1\b\f2\fs23 reduceByKey\b0\cell\b aggregateByKey\b0\cell\b groupByKey\b0\cell\row\trowd\trgaph15\trleft-55\trrh481\trbrdrl\brdrs\brdrw10 \trbrdrt\brdrs\brdrw10 \trbrdrr\brdrs\brdrw10 \trbrdrb\brdrs\brdrw10 \trpaddl15\trpaddr15\trpaddfl3\trpaddfr3
\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx2874\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx6169\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx9291\pard\intbl\sl480\slmult1 Uses Combiner\cell Uses Combiner\cell Does not use combiner\cell\row\trowd\trgaph15\trleft-55\trrh1443\trbrdrl\brdrs\brdrw10 \trbrdrt\brdrs\brdrw10 \trbrdrr\brdrs\brdrw10 \trbrdrb\brdrs\brdrw10 \trpaddl15\trpaddr15\trpaddfl3\trpaddfr3
\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx2874\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx6169\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx9291\pard\intbl\sl480\slmult1 Take one parameter as function \endash  for seqOp and combOp\cell Take 2 parameters as functions \endash  one for seqOp and other for combOp\cell No parameters as functions. Generally followed by map or flatMap\cell\row\trowd\trgaph15\trleft-55\trrh481\trbrdrl\brdrs\brdrw10 \trbrdrt\brdrs\brdrw10 \trbrdrr\brdrs\brdrw10 \trbrdrb\brdrs\brdrw10 \trpaddl15\trpaddr15\trpaddfl3\trpaddfr3
\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx2874\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx6169\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx9291\pard\intbl\sl480\slmult1 Implicit Combiner\cell Explicit\~Combiner\cell No combiner\cell\row\trowd\trgaph15\trleft-55\trrh1443\trbrdrl\brdrs\brdrw10 \trbrdrt\brdrs\brdrw10 \trbrdrr\brdrs\brdrw10 \trbrdrb\brdrs\brdrw10 \trpaddl15\trpaddr15\trpaddfl3\trpaddfr3
\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx2874\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx6169\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx9291\pard\intbl\sl480\slmult1 seqOp or combiner logic are same as combOp or final reduce logic\cell seqOp or combiner logic are different\~from combOp or final reduce logic\cell No combiner\cell\row\trowd\trgaph15\trleft-55\trrh962\trbrdrl\brdrs\brdrw10 \trbrdrt\brdrs\brdrw10 \trbrdrr\brdrs\brdrw10 \trbrdrb\brdrs\brdrw10 \trpaddl15\trpaddr15\trpaddfl3\trpaddfr3
\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx2874\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx6169\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx9291\pard\intbl\sl480\slmult1 Input and output value type need to be\~same\cell Input and output value type\~can be different\cell No parameters\cell\row\trowd\trgaph15\trleft-55\trrh948\trbrdrl\brdrs\brdrw10 \trbrdrt\brdrs\brdrw10 \trbrdrr\brdrs\brdrw10 \trbrdrb\brdrs\brdrw10 \trpaddl15\trpaddr15\trpaddfl3\trpaddfr3
\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx2874\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx6169\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx9291\pard\intbl\sl480\slmult1 Performance is high for aggregations\cell Performance is high for aggregations\cell Relatively slow for aggregations\cell\row\trowd\trgaph15\trleft-55\trrh962\trbrdrl\brdrs\brdrw10 \trbrdrt\brdrs\brdrw10 \trbrdrr\brdrs\brdrw10 \trbrdrb\brdrs\brdrw10 \trpaddl15\trpaddr15\trpaddfl3\trpaddfr3
\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx2874\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx6169\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx9291\pard\intbl\sl480\slmult1 Only aggregations\cell Only aggregations\cell Any by key transformation \endash  aggregation, sorting, ranking etc.\cell\row\trowd\trgaph15\trleft-55\trrh1457\trbrdrl\brdrs\brdrw10 \trbrdrt\brdrs\brdrw10 \trbrdrr\brdrs\brdrw10 \trbrdrb\brdrs\brdrw10 \trpaddl15\trpaddr15\trpaddfl3\trpaddfr3
\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx2874\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx6169\clvertalc\clcbpat2\clbrdrl\brdrw10\brdrs\clbrdrt\brdrw10\brdrs\clbrdrr\brdrw10\brdrs\clbrdrb\brdrw10\brdrs \cellx9291\pard\intbl\sl480\slmult1 eg: sum, min, max etc\cell eg: average\cell eg: any aggregation is possible, but not preferred for performance reasons\cell\row\pard\nowidctlpar\cf0\f0\fs22\par
scala> val path="/user/cloudera/retail_db"\par
path: String = /user/cloudera/retail_db\par
scala> sc.textFile(path+"/order_items").take(20).foreach(println)\par
1,1,957,1,299.98,299.98\par
2,2,1073,1,199.99,199.99\par
3,2,502,5,250.0,50.0\par
4,2,403,1,129.99,129.99\par
5,4,897,2,49.98,24.99\par
6,4,365,5,299.95,59.99\par
7,4,502,3,150.0,50.0\par
8,4,1014,4,199.92,49.98\par
9,5,957,1,299.98,299.98\par
10,5,365,5,299.95,59.99\par
\par
\b order_id->4\b0\par
,order_id,,order_id_subtotal,\par
5,4,897,2,49.98,24.99\par
6,4,365,5,299.95,59.99\par
7,4,502,3,150.0,50.0\par
8,4,1014,4,199.92,49.98\par
\par
total revenue for order_id 4 -> 49.98+299.95+150.0+199.92 = 699.85\par
\par
For getting the total of each order_id, first we are extracting order_id and order_id_subtotal using map function and then apply reduceByKey function\par
scala> val orderItems = sc.textFile(path+"/order_items").map(orderItem => (orderItem.split(",")(1).toInt,orderItem.split(",")(4).toFloat))\par
orderItems: org.apache.spark.rdd.RDD[(Int, Float)] = MapPartitionsRDD[12] at map at <console>:29\par
scala> orderItems.take(10).foreach(println)\par
(1,299.98)\par
(2,199.99)\par
(2,250.0)\par
(2,129.99)\par
(4,49.98)\par
(4,299.95)\par
(4,150.0)\par
(4,199.92)\par
(5,299.98)\par
(5,299.95)\par
\par
scala> orderItems.reduceByKey((total,value) => total + value).take(10).foreach(println)\par
(65722,1319.8899)\par
(68522,329.99)\par
(23776,329.98)\par
(34207,219.94)\par
(53499,284.88998)\par
(32676,719.91003)\par
(53926,219.97)\par
(4926,939.85)\par
(38926,1049.9)\par
(51620,999.85004)\par
\lang9\par
scala> orderItems.reduceByKey((total,value) => total + value).filter(rec => rec._1 == 4).collect.foreach(println)\par
(4,699.85004)\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
}
 